{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **《《《 YOLO 》》》**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-11T04:57:54.459732Z",
     "iopub.status.busy": "2025-02-11T04:57:54.459507Z",
     "iopub.status.idle": "2025-02-11T04:59:21.833478Z",
     "shell.execute_reply": "2025-02-11T04:59:21.832565Z",
     "shell.execute_reply.started": "2025-02-11T04:57:54.459710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Error opening archive: Failed to open '/kaggle/input/ultralytics-for-offline-install/archive.tar.gz'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ./packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Location './packages' is ignored: it is either a non-existing path or lacks a specific scheme.\n",
      "ERROR: Could not find a version that satisfies the requirement ultralytics (from versions: none)\n",
      "ERROR: No matching distribution found for ultralytics\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '/kaggle/working/wheel_file/asciitree-0.3.3/asciitree-0.3.3': Expected package name at the start of dependency specifier\n",
      "    /kaggle/working/wheel_file/asciitree-0.3.3/asciitree-0.3.3\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/working/wheel_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Location '/kaggle/working/wheel_file' is ignored: it is either a non-existing path or lacks a specific scheme.\n",
      "ERROR: Could not find a version that satisfies the requirement zarr (from versions: none)\n",
      "ERROR: No matching distribution found for zarr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/working/wheel_file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Location '/kaggle/working/wheel_file' is ignored: it is either a non-existing path or lacks a specific scheme.\n",
      "ERROR: Could not find a version that satisfies the requirement connected-components-3d (from versions: none)\n",
      "ERROR: No matching distribution found for connected-components-3d\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: '/kaggle/input/czii-cryoet-dependencies/requirements.txt'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m deps_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/czii-cryoet-dependencies\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m pip install -q --no-index --find-links \u001b[39m\u001b[38;5;132;01m{deps_path}\u001b[39;00m\u001b[38;5;124m --requirement \u001b[39m\u001b[38;5;132;01m{deps_path}\u001b[39;00m\u001b[38;5;124m/requirements.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytz\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightning'"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "!tar xfvz /kaggle/input/ultralytics-for-offline-install/archive.tar.gz\n",
    "!pip install --no-index --find-links=./packages ultralytics\n",
    "!rm -rf ./packages\n",
    "try:\n",
    "    import zarr\n",
    "except: \n",
    "    !cp -r '/kaggle/input/hengck-czii-cryo-et-01/wheel_file' '/kaggle/working/'\n",
    "    !pip install /kaggle/working/wheel_file/asciitree-0.3.3/asciitree-0.3.3\n",
    "    !pip install --no-index --find-links=/kaggle/working/wheel_file zarr\n",
    "    !pip install --no-index --find-links=/kaggle/working/wheel_file connected-components-3d\n",
    "from typing import List, Tuple, Union\n",
    "deps_path = '/kaggle/input/czii-cryoet-dependencies'\n",
    "! pip install -q --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt\n",
    "import lightning.pytorch as pl\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/hengck-czii-cryo-et-01')\n",
    "from czii_helper import *\n",
    "from dataset import *\n",
    "from model2 import *\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:59:21.834784Z",
     "iopub.status.busy": "2025-02-11T04:59:21.834462Z",
     "iopub.status.idle": "2025-02-11T04:59:24.206504Z",
     "shell.execute_reply": "2025-02-11T04:59:24.205619Z",
     "shell.execute_reply.started": "2025-02-11T04:59:21.834753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import zarr\n",
    "from scipy.spatial import cKDTree\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:59:24.207920Z",
     "iopub.status.busy": "2025-02-11T04:59:24.207598Z",
     "iopub.status.idle": "2025-02-11T04:59:25.178526Z",
     "shell.execute_reply": "2025-02-11T04:59:25.177586Z",
     "shell.execute_reply.started": "2025-02-11T04:59:24.207890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = '/kaggle/input/czii-yolo-l-trained-with-synthetic-data/best_synthetic.pt'\n",
    "model = YOLO(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:59:25.179792Z",
     "iopub.status.busy": "2025-02-11T04:59:25.179501Z",
     "iopub.status.idle": "2025-02-11T04:59:25.213898Z",
     "shell.execute_reply": "2025-02-11T04:59:25.213328Z",
     "shell.execute_reply.started": "2025-02-11T04:59:25.179770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "runs_path = '/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns/*'\n",
    "runs = sorted(glob.glob(runs_path))\n",
    "runs = [os.path.basename(run) for run in runs]\n",
    "sp = len(runs)//2\n",
    "runs1 = runs[:sp]\n",
    "runs1[:5]\n",
    "\n",
    "#add by @minfuka\n",
    "runs2 = runs[sp:]\n",
    "runs2[:5]\n",
    "\n",
    "#add by @minfuka\n",
    "assert torch.cuda.device_count() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:59:25.214741Z",
     "iopub.status.busy": "2025-02-11T04:59:25.214550Z",
     "iopub.status.idle": "2025-02-11T04:59:25.219126Z",
     "shell.execute_reply": "2025-02-11T04:59:25.218258Z",
     "shell.execute_reply.started": "2025-02-11T04:59:25.214724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "particle_names = [\n",
    "    'apo-ferritin',\n",
    "    'beta-amylase',\n",
    "    'beta-galactosidase',\n",
    "    'ribosome',\n",
    "    'thyroglobulin',\n",
    "    'virus-like-particle'\n",
    "]\n",
    "\n",
    "particle_to_index = {\n",
    "    'apo-ferritin': 0,\n",
    "    'beta-amylase': 1,\n",
    "    'beta-galactosidase': 2,\n",
    "    'ribosome': 3,\n",
    "    'thyroglobulin': 4,\n",
    "    'virus-like-particle': 5\n",
    "}\n",
    "\n",
    "index_to_particle = {index: name for name, index in particle_to_index.items()}\n",
    "\n",
    "particle_radius = {\n",
    "    'apo-ferritin': 70,\n",
    "    'beta-amylase': 75,\n",
    "    'beta-galactosidase': 95,\n",
    "    'ribosome': 150,\n",
    "    'thyroglobulin': 135,\n",
    "    'virus-like-particle': 145,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:59:25.220939Z",
     "iopub.status.busy": "2025-02-11T04:59:25.220744Z",
     "iopub.status.idle": "2025-02-11T04:59:25.240459Z",
     "shell.execute_reply": "2025-02-11T04:59:25.239689Z",
     "shell.execute_reply.started": "2025-02-11T04:59:25.220922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# add by @sesasj\n",
    "class UnionFind:\n",
    "    def __init__(self, size):\n",
    "        self.parent = np.arange(size)\n",
    "        self.rank = np.zeros(size, dtype=int)\n",
    "\n",
    "    def find(self, u):\n",
    "        if self.parent[u] != u:\n",
    "            self.parent[u] = self.find(self.parent[u])  \n",
    "        return self.parent[u]\n",
    "\n",
    "    def union(self, u, v):\n",
    "        u_root = self.find(u)\n",
    "        v_root = self.find(v)\n",
    "        if u_root == v_root:\n",
    "            return\n",
    "            \n",
    "        if self.rank[u_root] < self.rank[v_root]:\n",
    "            self.parent[u_root] = v_root\n",
    "        else:\n",
    "            self.parent[v_root] = u_root\n",
    "            if self.rank[u_root] == self.rank[v_root]:\n",
    "                self.rank[u_root] += 1\n",
    "\n",
    "class PredictionAggregator:\n",
    "    def __init__(self, first_conf=0.2, conf_coef=0.75):\n",
    "        self.first_conf = first_conf\n",
    "        self.conf_coef = conf_coef\n",
    "        self.particle_confs = np.array([0.5, 0.0, 0.2, 0.5, 0.2, 0.5])\n",
    "        \n",
    "    def convert_to_8bit(self, volume):\n",
    "        lower, upper = np.percentile(volume, (0.5, 99.5))\n",
    "        clipped = np.clip(volume, lower, upper)\n",
    "        scaled = ((clipped - lower) / (upper - lower + 1e-12) * 255).astype(np.uint8)\n",
    "        return scaled\n",
    "\n",
    "    def make_predictions(self, run_id, model, device_no):\n",
    "        volume_path = f'/kaggle/input/czii-cryo-et-object-identification/test/static/ExperimentRuns/{run_id}/VoxelSpacing10.000/denoised.zarr'\n",
    "        volume = zarr.open(volume_path, mode='r')[0]\n",
    "        volume_8bit = self.convert_to_8bit(volume)\n",
    "        num_slices = volume_8bit.shape[0]\n",
    "\n",
    "        detections = {\n",
    "            'particle_type': [],\n",
    "            'confidence': [],\n",
    "            'x': [],\n",
    "            'y': [],\n",
    "            'z': []\n",
    "        }\n",
    "\n",
    "        for slice_idx in range(num_slices):\n",
    "            \n",
    "            img = volume_8bit[slice_idx]\n",
    "            input_image = cv2.resize(np.stack([img]*3, axis=-1), (640, 640))\n",
    "\n",
    "            results = model.predict(\n",
    "                input_image,\n",
    "                save=False,\n",
    "                imgsz=640,\n",
    "                conf=self.first_conf,\n",
    "                device=device_no,\n",
    "                batch=1,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is None:\n",
    "                    continue\n",
    "                cls = boxes.cls.cpu().numpy().astype(int)\n",
    "                conf = boxes.conf.cpu().numpy()\n",
    "                xyxy = boxes.xyxy.cpu().numpy()\n",
    "\n",
    "                xc = ((xyxy[:, 0] + xyxy[:, 2]) / 2.0) * 10 * (63/64) # 63/64 because of the resize\n",
    "                yc = ((xyxy[:, 1] + xyxy[:, 3]) / 2.0) * 10 * (63/64)\n",
    "                zc = np.full(xc.shape, slice_idx * 10 + 5)\n",
    "\n",
    "                particle_types = [index_to_particle[c] for c in cls]\n",
    "\n",
    "                detections['particle_type'].extend(particle_types)\n",
    "                detections['confidence'].extend(conf)\n",
    "                detections['x'].extend(xc)\n",
    "                detections['y'].extend(yc)\n",
    "                detections['z'].extend(zc)\n",
    "\n",
    "        if not detections['particle_type']:\n",
    "            return pd.DataFrame()  \n",
    "\n",
    "        particle_types = np.array(detections['particle_type'])\n",
    "        confidences = np.array(detections['confidence'])\n",
    "        xs = np.array(detections['x'])\n",
    "        ys = np.array(detections['y'])\n",
    "        zs = np.array(detections['z'])\n",
    "\n",
    "        aggregated_data = []\n",
    "\n",
    "        for idx, particle in enumerate(particle_names):\n",
    "            if particle == 'beta-amylase':\n",
    "                continue \n",
    "\n",
    "            mask = (particle_types == particle)\n",
    "            if not np.any(mask):\n",
    "                continue  \n",
    "                \n",
    "            particle_confidences = confidences[mask]\n",
    "            particle_xs = xs[mask]\n",
    "            particle_ys = ys[mask]\n",
    "            particle_zs = zs[mask]\n",
    "            # -------------modified by @sersasj ------------------------\n",
    "            coords = np.vstack((particle_xs, particle_ys, particle_zs)).T\n",
    "\n",
    "           \n",
    "            z_distance = 30 # How many slices can you \"jump\" to aggregate predictions 10 = 1, 20 = 2...\n",
    "            xy_distance = 20 # xy_tol_p2 in original code by ITK8191\n",
    "            \n",
    "            max_distance = math.sqrt(z_distance**2 + xy_distance**2)\n",
    "            tree = cKDTree(coords)            \n",
    "            pairs = tree.query_pairs(r=max_distance, p=2)\n",
    "\n",
    "            \n",
    "            uf = UnionFind(len(coords))\n",
    "            \n",
    "            coords_xy = coords[:, :2]\n",
    "            coords_z = coords[:, 2]\n",
    "            for u, v in pairs:\n",
    "                z_diff = abs(coords_z[u] - coords_z[v])\n",
    "                if z_diff > z_distance:\n",
    "                    continue  \n",
    "\n",
    "                xy_diff = np.linalg.norm(coords_xy[u] - coords_xy[v])\n",
    "                if xy_diff > xy_distance:\n",
    "                    continue  \n",
    "\n",
    "                uf.union(u, v)\n",
    "\n",
    "            roots = np.array([uf.find(i) for i in range(len(coords))])\n",
    "            unique_roots, inverse_indices, counts = np.unique(roots, return_inverse=True, return_counts=True)\n",
    "            conf_sums = np.bincount(inverse_indices, weights=particle_confidences)\n",
    "            \n",
    "            aggregated_confidences = conf_sums / (counts ** self.conf_coef)\n",
    "            cluster_per_particle = [4,1,2,9,4,8]\n",
    "            valid_clusters = (counts >= cluster_per_particle[idx]) & (aggregated_confidences > self.particle_confs[idx])\n",
    "\n",
    "            if not np.any(valid_clusters):\n",
    "                continue  \n",
    "\n",
    "            cluster_ids = unique_roots[valid_clusters]\n",
    "\n",
    "            centers_x = np.bincount(inverse_indices, weights=particle_xs) / counts\n",
    "            centers_y = np.bincount(inverse_indices, weights=particle_ys) / counts\n",
    "            centers_z = np.bincount(inverse_indices, weights=particle_zs) / counts\n",
    "\n",
    "            centers_x = centers_x[valid_clusters]\n",
    "            centers_y = centers_y[valid_clusters]\n",
    "            centers_z = centers_z[valid_clusters]\n",
    "\n",
    "            aggregated_df = pd.DataFrame({\n",
    "                'experiment': [run_id] * len(centers_x),\n",
    "                'particle_type': [particle] * len(centers_x),\n",
    "                'x': centers_x,\n",
    "                'y': centers_y,\n",
    "                'z': centers_z\n",
    "            })\n",
    "\n",
    "            aggregated_data.append(aggregated_df)\n",
    "\n",
    "        if aggregated_data:\n",
    "            return pd.concat(aggregated_data, axis=0)\n",
    "        else:\n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:59:25.241900Z",
     "iopub.status.busy": "2025-02-11T04:59:25.241616Z",
     "iopub.status.idle": "2025-02-11T04:59:51.588857Z",
     "shell.execute_reply": "2025-02-11T04:59:51.587941Z",
     "shell.execute_reply.started": "2025-02-11T04:59:25.241873Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.28s/it]\n",
      "100%|██████████| 2/2 [00:25<00:00, 12.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated total prediction time for 500 runs: 4387.7762 seconds\n"
     ]
    }
   ],
   "source": [
    "# instance main class\n",
    "aggregator = PredictionAggregator(first_conf=0.22,  conf_coef=0.39) #Update\n",
    "aggregated_results = []\n",
    "#add by @minfuka\n",
    "from concurrent.futures import ProcessPoolExecutor #add by @minfuka\n",
    "\n",
    "#add by @minfuka\n",
    "def inference(runs, model, device_no):\n",
    "    subs = []\n",
    "    for r in tqdm(runs, total=len(runs)):\n",
    "        df = aggregator.make_predictions(r, model, device_no)\n",
    "        subs.append(df)\n",
    "    \n",
    "    return subs\n",
    "start_time = time.time()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "    results = list(executor.map(inference, (runs1, runs2), (model, model), (\"0\", \"1\")))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "estimated_total_time = (end_time - start_time) / len(runs) * 500  \n",
    "print(f'estimated total prediction time for 500 runs: {estimated_total_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:59:51.590038Z",
     "iopub.status.busy": "2025-02-11T04:59:51.589758Z",
     "iopub.status.idle": "2025-02-11T04:59:51.596963Z",
     "shell.execute_reply": "2025-02-11T04:59:51.596170Z",
     "shell.execute_reply.started": "2025-02-11T04:59:51.590005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#change by @minfuka\n",
    "submission0 = pd.concat(results[0])\n",
    "submission1 = pd.concat(results[1])\n",
    "submission_ = pd.concat([submission0, submission1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:59:51.598289Z",
     "iopub.status.busy": "2025-02-11T04:59:51.597939Z",
     "iopub.status.idle": "2025-02-11T04:59:51.633200Z",
     "shell.execute_reply": "2025-02-11T04:59:51.632470Z",
     "shell.execute_reply.started": "2025-02-11T04:59:51.598258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission_.insert(0, 'id', range(len(submission_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Yolo, begin Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:59:51.634232Z",
     "iopub.status.busy": "2025-02-11T04:59:51.634018Z",
     "iopub.status.idle": "2025-02-11T05:01:02.474953Z",
     "shell.execute_reply": "2025-02-11T05:01:02.474208Z",
     "shell.execute_reply.started": "2025-02-11T04:59:51.634214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-662ef3c5dc41>:155: DeprecationWarning: config_type not found in config file, defaulting to filesystem\n",
      "  root = copick.from_file(copick_test_config_path)\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 139.30it/s]\n",
      "100%|██████████| 98/98 [00:12<00:00,  8.05it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 135.43it/s]\n",
      "100%|██████████| 98/98 [00:11<00:00,  8.29it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 144.11it/s]\n",
      "100%|██████████| 98/98 [00:12<00:00,  8.16it/s]\n"
     ]
    }
   ],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        spatial_dims: int = 3,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 7,\n",
    "        channels: Union[Tuple[int, ...], List[int]] = (48, 64, 80, 80),\n",
    "        strides: Union[Tuple[int, ...], List[int]] = (2, 2, 1),\n",
    "        num_res_units: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = UNet(\n",
    "            spatial_dims=self.hparams.spatial_dims,\n",
    "            in_channels=self.hparams.in_channels,\n",
    "            out_channels=self.hparams.out_channels,\n",
    "            channels=self.hparams.channels,\n",
    "            strides=self.hparams.strides,\n",
    "            num_res_units=self.hparams.num_res_units,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "channels = (48, 64, 80, 80)\n",
    "strides_pattern = (2, 2, 1)\n",
    "num_res_units = 1\n",
    "def extract_3d_patches_minimal_overlap(arrays: List[np.ndarray], patch_size: int) -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n",
    "    if not arrays or not isinstance(arrays, list):\n",
    "        raise ValueError(\"Input must be a non-empty list of arrays\")\n",
    "    \n",
    "    # Verify all arrays have the same shape\n",
    "    shape = arrays[0].shape\n",
    "    if not all(arr.shape == shape for arr in arrays):\n",
    "        raise ValueError(\"All input arrays must have the same shape\")\n",
    "    \n",
    "    if patch_size > min(shape):\n",
    "        raise ValueError(f\"patch_size ({patch_size}) must be smaller than smallest dimension {min(shape)}\")\n",
    "    \n",
    "    m, n, l = shape\n",
    "    patches = []\n",
    "    coordinates = []\n",
    "    \n",
    "    # Calculate starting positions for each dimension\n",
    "    x_starts = calculate_patch_starts(m, patch_size)\n",
    "    y_starts = calculate_patch_starts(n, patch_size)\n",
    "    z_starts = calculate_patch_starts(l, patch_size)\n",
    "    \n",
    "    # Extract patches from each array\n",
    "    for arr in arrays:\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                for z in z_starts:\n",
    "                    patch = arr[\n",
    "                        x:x + patch_size,\n",
    "                        y:y + patch_size,\n",
    "                        z:z + patch_size\n",
    "                    ]\n",
    "                    patches.append(patch)\n",
    "                    coordinates.append((x, y, z))\n",
    "    \n",
    "    return patches, coordinates\n",
    "def reconstruct_array(patches: List[np.ndarray], \n",
    "                     coordinates: List[Tuple[int, int, int]], \n",
    "                     original_shape: Tuple[int, int, int]) -> np.ndarray:\n",
    "    reconstructed = np.zeros(original_shape, dtype=np.int64)  # To track overlapping regions\n",
    "    \n",
    "    patch_size = patches[0].shape[0]\n",
    "    \n",
    "    for patch, (x, y, z) in zip(patches, coordinates):\n",
    "        reconstructed[\n",
    "            x:x + patch_size,\n",
    "            y:y + patch_size,\n",
    "            z:z + patch_size\n",
    "        ] = patch\n",
    "        \n",
    "    \n",
    "    return reconstructed\n",
    "def calculate_patch_starts(dimension_size: int, patch_size: int) -> List[int]:\n",
    "    if dimension_size <= patch_size:\n",
    "        return [0]\n",
    "        \n",
    "    # Calculate number of patches needed\n",
    "    n_patches = np.ceil(dimension_size / patch_size)\n",
    "    \n",
    "    if n_patches == 1:\n",
    "        return [0]\n",
    "    \n",
    "    # Calculate overlap\n",
    "    total_overlap = (n_patches * patch_size - dimension_size) / (n_patches - 1)\n",
    "    \n",
    "    # Generate starting positions\n",
    "    positions = []\n",
    "    for i in range(int(n_patches)):\n",
    "        pos = int(i * (patch_size - total_overlap))\n",
    "        if pos + patch_size > dimension_size:\n",
    "            pos = dimension_size - patch_size\n",
    "        if pos not in positions:  # Avoid duplicates\n",
    "            positions.append(pos)\n",
    "    \n",
    "    return positions\n",
    "import pandas as pd\n",
    "\n",
    "def dict_to_df(coord_dict, experiment_name):\n",
    "    # Create lists to store data\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process each label and its coordinates\n",
    "    for label, coords in coord_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "    \n",
    "    # Concatenate all coordinates\n",
    "    all_coords = np.vstack(all_coords)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'experiment': experiment_name,\n",
    "        'particle_type': all_labels,\n",
    "        'x': all_coords[:, 0],\n",
    "        'y': all_coords[:, 1],\n",
    "        'z': all_coords[:, 2]\n",
    "    })\n",
    "\n",
    "    \n",
    "    return df\n",
    "from typing import List, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    EnsureChannelFirstd, \n",
    "    Orientationd,  \n",
    "    AsDiscrete,  \n",
    "    RandFlipd, \n",
    "    RandRotate90d, \n",
    "    NormalizeIntensityd,\n",
    "    RandCropByLabelClassesd,\n",
    ")\n",
    "TRAIN_DATA_DIR = \"/kaggle/input/create-numpy-dataset-exp-name\"\n",
    "import json\n",
    "copick_config_path = TRAIN_DATA_DIR + \"/copick.config\"\n",
    "\n",
    "with open(copick_config_path) as f:\n",
    "    copick_config = json.load(f)\n",
    "\n",
    "copick_config['static_root'] = '/kaggle/input/czii-cryo-et-object-identification/test/static'\n",
    "\n",
    "copick_test_config_path = 'copick_test.config'\n",
    "\n",
    "with open(copick_test_config_path, 'w') as outfile:\n",
    "    json.dump(copick_config, outfile)\n",
    "import copick\n",
    "\n",
    "root = copick.from_file(copick_test_config_path)\n",
    "\n",
    "copick_user_name = \"copickUtils\"\n",
    "copick_segmentation_name = \"paintedPicks\"\n",
    "voxel_size = 11\n",
    "tomo_type = \"denoised\"\n",
    "inference_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\")\n",
    "])\n",
    "import cc3d\n",
    "\n",
    "id_to_name = {1: \"apo-ferritin\", \n",
    "              2: \"beta-amylase\",\n",
    "              3: \"beta-galactosidase\", \n",
    "              4: \"ribosome\", \n",
    "              5: \"thyroglobulin\", \n",
    "              6: \"virus-like-particle\"}\n",
    "BLOB_THRESHOLD = 300\n",
    "CERTAINTY_THRESHOLD = 0.05\n",
    "\n",
    "classes = [1, 2, 3, 4, 5, 6]\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cc3d\n",
    "from monai.data import CacheDataset\n",
    "from monai.transforms import Compose, EnsureType\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import TverskyLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "def load_models(model_paths):\n",
    "    models = []\n",
    "    for model_path in model_paths:\n",
    "        channels = (48, 64, 80, 80)\n",
    "        strides_pattern = (2, 2, 1)       \n",
    "        num_res_units = 1\n",
    "        learning_rate = 1e-3\n",
    "        num_epochs = 100\n",
    "        model = Model(channels=channels, strides=strides_pattern, num_res_units=num_res_units)\n",
    "        \n",
    "        weights =torch.load(model_path)['state_dict']\n",
    "        model.load_state_dict(weights)\n",
    "        model.to('cuda')\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    '/kaggle/input/cziials-a-230-unet/UNet-Model-val_metric0.450.ckpt',\n",
    "]\n",
    "\n",
    "\n",
    "models = load_models(model_paths)\n",
    "def ensemble_prediction_tta(models, input_tensor, threshold=0.5):\n",
    "    probs_list = []\n",
    "    data_copy0 = input_tensor.clone()\n",
    "    data_copy0=torch.flip(data_copy0, dims=[2])\n",
    "    data_copy1 = input_tensor.clone()\n",
    "    data_copy1=torch.flip(data_copy1, dims=[3])\n",
    "    data_copy2 = input_tensor.clone()\n",
    "    data_copy2=torch.flip(data_copy2, dims=[4])\n",
    "    data_copy3 = input_tensor.clone()\n",
    "    data_copy3 = data_copy3.rot90(1, dims=[3, 4])\n",
    "    with torch.no_grad():\n",
    "        model_output0 = model(input_tensor)\n",
    "        model_output1 = model(data_copy0)\n",
    "        model_output1=torch.flip(model_output1, dims=[2])\n",
    "        model_output2 = model(data_copy1)\n",
    "        model_output2=torch.flip(model_output2, dims=[3])\n",
    "        model_output3 = model(data_copy2)\n",
    "        model_output3=torch.flip(model_output3, dims=[4])\n",
    "        probs0 = torch.softmax(model_output0[0], dim=0)\n",
    "        probs1 = torch.softmax(model_output1[0], dim=0)\n",
    "        probs2 = torch.softmax(model_output2[0], dim=0)\n",
    "        probs3 = torch.softmax(model_output3[0], dim=0)\n",
    "        probs_list.append(probs0)\n",
    "        probs_list.append(probs1)\n",
    "        probs_list.append(probs2)\n",
    "        probs_list.append(probs3)\n",
    "    avg_probs = torch.mean(torch.stack(probs_list), dim=0)\n",
    "    thresh_probs = avg_probs > threshold\n",
    "    _, max_classes = thresh_probs.max(dim=0)\n",
    "    return max_classes\n",
    "sub=[]\n",
    "for model in models:\n",
    "    with torch.no_grad():\n",
    "        location_df = []\n",
    "        for run in root.runs:\n",
    "            tomo = run.get_voxel_spacing(10)\n",
    "            tomo = tomo.get_tomogram(tomo_type).numpy()\n",
    "            tomo_patches, coordinates = extract_3d_patches_minimal_overlap([tomo], 96)\n",
    "            tomo_patched_data = [{\"image\": img} for img in tomo_patches]\n",
    "            tomo_ds = CacheDataset(data=tomo_patched_data, transform=inference_transforms, cache_rate=1.0)\n",
    "            pred_masks = []\n",
    "            for i in tqdm(range(len(tomo_ds))):\n",
    "                input_tensor = tomo_ds[i]['image'].unsqueeze(0).to(\"cuda\")\n",
    "                max_classes = ensemble_prediction_tta(models, input_tensor, threshold=CERTAINTY_THRESHOLD)\n",
    "                pred_masks.append(max_classes.cpu().numpy())\n",
    "            reconstructed_mask = reconstruct_array(pred_masks, coordinates, tomo.shape)\n",
    "            location = {}\n",
    "            for c in classes:\n",
    "                cc = cc3d.connected_components(reconstructed_mask == c)\n",
    "                stats = cc3d.statistics(cc)\n",
    "                zyx = stats['centroids'][1:] * 10.012444  # 转换单位\n",
    "                zyx_large = zyx[stats['voxel_counts'][1:] > BLOB_THRESHOLD]\n",
    "                xyz = np.ascontiguousarray(zyx_large[:, ::-1])\n",
    "                location[id_to_name[c]] = xyz\n",
    "            df = dict_to_df(location, run.name)\n",
    "            location_df.append(df)\n",
    "        location_df = pd.concat(location_df)\n",
    "        location_df.insert(loc=0, column='id', value=np.arange(len(location_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBScan. Merge the model's outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T05:01:02.476901Z",
     "iopub.status.busy": "2025-02-11T05:01:02.475991Z",
     "iopub.status.idle": "2025-02-11T05:01:04.076846Z",
     "shell.execute_reply": "2025-02-11T05:01:04.076160Z",
     "shell.execute_reply.started": "2025-02-11T05:01:02.476869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    " #### merge yolo and unet results\n",
    "  = pd.concat([submission_,location_df], ignore_index=True)\n",
    "\n",
    "particle_names = ['apo-ferritin', 'beta-amylase', 'beta-galactosidase', 'ribosome', 'thyroglobulin', 'virus-like-particle']\n",
    "particle_radius = {\n",
    "    'apo-ferritin': 65,\n",
    "    'beta-amylase': 65,\n",
    "    'beta-galactosidase': 95,\n",
    "    'ribosome': 150,\n",
    "    'thyroglobulin': 130,\n",
    "    'virus-like-particle': 135,\n",
    "}\n",
    "\n",
    "final = []\n",
    "for pidx, p in enumerate(particle_names):\n",
    "    pdf = df[df['particle_type'] == p].reset_index(drop=True)\n",
    "    p_rad = particle_radius[p]\n",
    "    \n",
    "    grouped = pdf.groupby(['experiment'])\n",
    "    \n",
    "    for exp, group in grouped:\n",
    "        group = group.reset_index(drop=True)\n",
    "        \n",
    "        coords = group[['x', 'y', 'z']].values\n",
    "        db = DBSCAN(eps=p_rad, min_samples=2, metric='euclidean').fit(coords)\n",
    "        labels = db.labels_\n",
    "        \n",
    "        group['cluster'] = labels\n",
    "        \n",
    "        for cluster_id in np.unique(labels):\n",
    "            if cluster_id == -1:\n",
    "                continue\n",
    "            \n",
    "            cluster_points = group[group['cluster'] == cluster_id]\n",
    "            \n",
    "            avg_x = cluster_points['x'].mean()\n",
    "            avg_y = cluster_points['y'].mean()\n",
    "            avg_z = cluster_points['z'].mean()\n",
    "            \n",
    "            group.loc[group['cluster'] == cluster_id, ['x', 'y', 'z']] = avg_x, avg_y, avg_z\n",
    "            group = group.drop_duplicates(subset=['x', 'y', 'z'])\n",
    "        final.append(group)\n",
    "\n",
    "df_save = pd.concat(final, ignore_index=True)\n",
    "df_save = df_save.drop(columns=['cluster'])\n",
    "df_save = df_save.sort_values(by=['experiment', 'particle_type']).reset_index(drop=True)\n",
    "df_save['id'] = np.arange(0, len(df_save))\n",
    "df_save.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10033515,
     "sourceId": 84969,
     "sourceType": "competition"
    },
    {
     "datasetId": 6052780,
     "sourceId": 9862305,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6040935,
     "sourceId": 9867543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6465904,
     "sourceId": 10445850,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6484063,
     "sourceId": 10471985,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 206640467,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211097053,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
